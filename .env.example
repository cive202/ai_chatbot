# LLM Provider Configuration
# Options: 'ollama' or 'vllm'
LLM_PROVIDER=ollama

# Ollama Configuration
# CRITICAL: DO NOT USE 'llama3:latest' - Unquantized model (~12GB) will crash RTX 3060
# REQUIRED: Use quantized models (Q4_K_M or Q5_K_M) for RTX 3060 12GB VRAM
# Valid options:
#   - llama3:8b-instruct-q4_K_M (~4.5GB) - Recommended for RTX 3060
#   - llama3:8b-instruct-q5_K_M (~5.5GB) - Better quality, slightly more VRAM
#   - llama3:8b-instruct-q4_K_S (~3.8GB) - Lower VRAM usage
OLLAMA_API_URL=http://localhost:11434
OLLAMA_MODEL=llama3:8b-instruct-q4_K_M

# vLLM Configuration (if using vLLM provider)
VLLM_API_URL=http://localhost:8000/v1/chat/completions
VLLM_MODEL=meta-llama/Meta-Llama-3-8B-Instruct

# Logging Configuration
LOG_LEVEL=INFO
